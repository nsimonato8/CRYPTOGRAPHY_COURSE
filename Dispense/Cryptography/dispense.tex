\documentclass[12pt, a4paper, english]{report}
\usepackage[utf8]{inputenc}
\usepackage{makecell}
\usepackage{fourier}
\usepackage{caption}
\usepackage{amssymb}
\usepackage[linesnumbered]{algorithm2e} %,lined,boxed,commentsnumbered
\usepackage{subcaption}
\usepackage{epigraph}
\usepackage{tikz}
\usepackage{babel}
\usepackage{amsfonts}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{csquotes}
\usepackage{underscore}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{thmtools}
\usepackage{biblatex}
\usepackage[rightcaption]{sidecap}
\usepackage{graphicx}
\usepackage{tabularx}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}

\DeclareMathOperator*{\argmin}{arg\,min}

\graphicspath{ {img/} }

\author{NiccolÃ² Simonato}
\title{Cryptography: notes}

\begin{document}

\maketitle

\tableofcontents

\chapter{Efficient implementations of elementary operations}
\section{Notation}
\begin{itemize}

    \item Let $b$ be a numeric base.
    \item Let $n$ be a number in $N$.
    \item Length of a number: $l_{b}(n)$, $k$. It's equal to $log(n)$.
    \item $(a,b)$ is the Maximum Common Divisor of $a,b$.
    \item Let $n \in N$: $n = (d_{k-1}, d_{k-2}, \dots, d_{1}, d_{0})$\footnote{$d_{k-1} \neq 0$}.
    \item $\varphi(n)$: the number of elements $a$ in $[1,n]$ such that $(a,n) = 1$.
    \item $\equiv_{p}$ is the equivalence in base $p$. Ex.: $5 \equiv_{3} = 5 \bmod 3 = 2 $.
    \item Let $\mathbb{Z}_{n}[X]$ be the set of polynomials in $X$ with coefficients in $\mathbb{Z}_{n}$.
\end{itemize}


\section{Classification of the algorithms' complexity}
In order to better identify the classes of complexity of the algorithms, the following 3 classes are defined:
\begin{itemize}
    \item Polynomial time: $O(log^{\alpha}(n))$ bit operations, where $\alpha > 0$.
    \item Exponential time: $O(exp(c \cdot log(n)))$ bit operations, where $c > 0$.
    \item Sub-exponential time: $O(exp(c \cdot log(n))^{\alpha})$ bit operations, where $c > 0, \alpha \in ]0, 1[$.
\item
\end{itemize}



\section{Basic bit operations}
\subsection{Sum of 3 bits - 3-bit-sum}
Given $n_{1}, n_{2}$ their sum produces $n_{1} + n_{2}$ and their carry. \newline
Since $n_{1}, n_{2} \in [0,1]$, then this operation can be done in $O(1)$.

\subsection{Summation of 2 numbers}
Given $n_{1}, n_{2}$ their sum produces $n_{1} + n_{2}$. \newline
Since the sum is computed bit by bit, the 3-bit-sum is performed\\$max\{lenght(n_{1}), length(n_{2})\}$ times.\newline
Each time the carry on of the previous sum is added to the two digits. \newline
This operation has then complexity:\\$O(max\{lenght(n_{1}), length(n_{2})\}) = O(max\{log(n_{1}), log(n_{2})\})$

\subsection{Summation of $n$ numbers}
The summation of $n$ numbers is simply the sum of two numbers, but performed $n - 1$ times. \newline
Let's assume that $\forall i \in [1,n]: M \geq a_{i}$. \newline
The complexity of this operation is then:\\$O((n-1) \cdot log(M)) = O(n)$. \newline

\subsection{Product of 2 numbers}
If we consider the classic implementation of the binary multiplication, that is just a sequence of summations. \newline
\begin{itemize}
    \item The number of summations to execute is equal to the length of the smallest number, $O(log(n))$.
    \item The maximum cost of a single summation is $O(log(m))$.
    \item Then, $T(m \cdot n) = O(log(m) \cdot log(n))$, but, if we consider the worst case\footnote{two numbers that are equally large}, that becomes $O(log^{2}(m))$.
\end{itemize}

\subsection{Division of 2 numbers}
Let's consider the division of two numbers $m, n$. This operations consists in \\ finding two numbers $q, r$ such that $m = q \cdot n + r$. \newline
This is achieved by performing a succession of subtractions, until the ending condition $0 \leq r < n$ is reached. \newline
\begin{itemize}
    \item Let's consider that the number of steps of this algorithm is $O(log(q))$.
    \item Moreover, $q \leq m \therefore \#steps = O(log(m))$.
    \item It's assumed that the cost of the single subtraction is $O(log(n))$.
    \item Then, $T(\frac{m}{n}) = O(log(n) \cdot log(m))$.
\end{itemize}

\subsection{Production of $n$ numbers}
Let's assume that $j \in [1, s+1]$ and $M = max(m_{j})$. \newline
The cost of the operation $\prod_{j = 1}^{s+1} m_{j}$ is then $O(s^{2} \cdot log^{2}(M))$. This will now be considered our inductive hypothesis.\newline
Proof by induction, on $s$:
\begin{itemize}
    \item (1) Base case: $T(m_{1} \cdot m_{2}) = O(log(m_{1}) \cdot log(m_{2})) = O(k_{1} \cdot k_{2}) \leq c \cdot k_{M}^{2}$.
    \item (2) Base case: $T(m_{1} \cdot m_{2} \cdot m_{3}) = T(m_{1} \cdot m_{2}) + T((m_{1} \cdot m_{2}) \cdot m_{3})$ \\ $\leq c \cdot k_{M}^{2} + c \cdot k_{m_{1} \cdot m_{2}} + k_{m_{3}}$ \\ $\leq c \cdot k_{M}^{2} + c \cdot k_{M^{2}} + k_{M}$
    \item Inductive step: we assume the inductive hypothesis to be true up to $s$. Then,
    \\ $T(\prod_{j = 1}^{s+1} m_{j}) = T([\prod_{j = 1}^{s} m_{j}] \cdot m_{s+1})$
    \\ $\leq c \cdot \sum_{j=1}^{s} (j \cdot k_{M}^{2})$
    \\ $= c \cdot k_{M}^{2} \cdot \frac{s \cdot (s-1)}{2}$
    \\ $= O(k_{M}^{2} \cdot s^{2})$
    \\ $= O(s^{2} \cdot log^{2}(M))$
\end{itemize}
\subsubsection{Applications}
\begin{itemize}
    \item An analogous dimonstration can be used to prove that $T(\prod_{j = 1}^{s+1} m_{j} \bmod n) = O(s \cdot log^{2}(M))$
    \item This proof can be used to show that $T(m!) = O(m \cdot log^{2}(m))$.
\end{itemize}

\section{Optimizations of more complex operations}
\subsection{Powers \& Modular Powers}
Let's consider what follows: $a^n = a \cdot a \cdot a \cdot \dots \cdot a$, where $a$ is repeated $n$ times.
\subsubsection{Trivial implementation}
The most trivial implementation would consists in computing the product $\prod_{j = 1}^{n} a$. This would imply a cost of $O(n^{2} \cdot log^{2}(a)))$.\newline
What follows is a suggestion that could improve the cost of this operation.
\subsubsection{Square \& Multiply method for scalars, modular powers}
Each number in $\mathbb{Z}$ can be represented in a binary notation. \newline
Let's consider $n = (b_{k-1}, b_{k-2}, \dots, b_{0}) = \sum_{i=0}^{k-1} b_{i} \cdot 2^{i}$. \newline
It is clear that we can spare a lot of computational resources by just calculating the powers of 2 and summing the ones that have $b_{i} = 1$. The following algorithm explains the procedure in detail.
\RestyleAlgo{ruled}
\begin{algorithm}
\caption{The Square \& Multiply Method}\label{alg:SquareMultiply}
$P \gets 1$\;
$M \gets m$\;
$A \gets a \bmod n$\;\label{instr_3}
\While{$M > 0$}{
    $q \gets \lfloor \frac{M}{2} \rfloor$\;\label{instr_5}
    $r \gets M - s \cdot q$\;\label{instr_6}
    \If{$r = 1$}{
        $P \gets P \cdot A \bmod n$\;\label{instr_8}
    }
    $A \gets A^{2} \bmod n$\;\label{instr_10}
    $M \gets q$\;

}
\Return{P}
\end{algorithm}
Let's compute the complexity of this algorithm:
\begin{itemize}
    \item All of the assignments $X \gets Y$ are implemented in $O(log(Y))$.
    \item The cost of \ref{instr_3} is $O(log(a) \cdot log(n))$, because it ensures that $A \leq n$.
    \item Instructions \ref{instr_5} and \ref{instr_6} can be executed in $O(log(m))$.
    \item Instrucion \ref{instr_8} can be executed in $O(log^{2}(n))$.
    \item The cost of \ref{instr_10} is $O(log^{2}(n))$, because it ensures that $A \leq n$.
    \item The loop is executed $log(m)$ times.
    \item The total cost of this algorithm is then $O(log(n) \cdot log(a) + log(m) \cdot (log^{2}(n) + log(m)))$ \\ $= O(log^{2}(m) + log(m) \cdot log(n))$.
\end{itemize}
This algorithm can be easily converted for the computation of non-modular powers by applying the following changes:
\RestyleAlgo{ruled}
\begin{algorithm}
    $A \gets a \bmod n$ $\Longrightarrow$ $A \gets a$\;\label{instr_3a}
    $P \gets P \cdot A \bmod n$ $\Longrightarrow$ $P \gets P \cdot A$\;\label{instr_8a}
    $A \gets A^{2} \bmod n$ $\Longrightarrow$ $A \gets A^{2}$\;\label{instr_10a}
\end{algorithm}
\subsubsection{Square \& Multiply method for polynomials}
Let's consider $\Re = \frac{\mathbb{Z}_{n}[x]}{x^{r} - 1}$. The modular powers of the elements in this set can be computed by using a variation of the Square \& Multiply method. \newline
\begin{itemize}
    \item Assume that $f, g \in \Re$.
    \item Let $h(x) = f(x) \cdot g(x) = \sum_{j = 0}^{2r - 2} h_{j} \cdot x^{j}$.
    \item Where $h(j) = (\sum_{i=0}^{j} f_{i} \cdot g_{j - i} \bmod n) \bmod n$.
    \item Then, $T(h_{j}) = O(j \cdot log^{2}(n))$.
    \item Then, $T(h(x)) = O(\sum_{j = 0}^{log^{2}(n)}) = O(r^{2} \cdot log^{2}(n))$.
\end{itemize}
This result will be useful in the following computations. \newline
Let's now consider $\frac{h(x)}{x^{r} - 1}$.
\begin{itemize}
    \item When $j > r - 1$, $h_{j} \cdot x^{j}$ does not take any part in the computations.
    \item When $j = r$, then, $\frac{h_{r} \cdot x^{r}}{x^{r} - 1 } = h_{r} + \frac{h_{r}}{x^{r} - 1}$ \\ Or, in other words: $h_{r} \cdot x^{r} \equiv_{x^{r} - 1} h_{r}$.
    \item In the other cases: $h_{r+i} \cdot x^{r+i} \equiv_{x^{r} - 1} h_{r-i} \cdot x^{r-i}$ for $1 \leq i \leq r - 2$.
\end{itemize}
%An image can be useful
Then, $h(x) \equiv_{(n, x^{r} - 1)} f(x) \cdot g(x) \equiv$ \\ $[\sum_{j=0}^{r - 2}((h_{j} + h_{r + j}) \bmod n) \cdot x^{j}] + h_{r-1} \cdot x^{r-1}$.\label{comp:partial_prod_pol} \newline

Therefore, $T(h(x) \bmod (n, x^{r} - 1)) = O(r^{2} \cdot log^{2}(n))$. \newline
Finally, we can analyze the complexity of the computation of the modular power $h(x)$ elevated to $n$. \newline
In order to optimize the use of the computational resources, we can use a variation of the Square \& Multiply method (See \ref{alg:SquareMultiply});
although, this time, the computation of the partial products will be conducted by using the previously explained procedure (See \ref{comp:partial_prod_pol}). \newline
The cost of this method would then be $T(\#Loops \cdot (h(x) \bmod (n, x^{r} - 1))) =$ \\ $O(log(n) \cdot r^{2} \cdot log(n)) = O(r^{2} \cdot log^{3}(n))$.

\subsection{Finding the $b$-expansion of $n$ ($n_{b}$)}
Let's consider the cost in bit operations of the conversion of a number $n$ to a new base $b$. \newline
The algorithm used will be the classical: a succession of divisions by $b$. \newline
\begin{itemize}
    \item Let's consider $r_{i} \in \{0, 1, \dots, b-1\}$.
    \item Let $n_{b} = (r_{k+1}, r_{k}, \dots, r_{1}, r_{0})$.
    \item Then: \begin{itemize}
                    \item $n = q_{0} \cdot b + r_{0}$
                    \item $q_{0} = q_{1} \cdot b + r_{1}$
                    \item $\dots$
                    \item $q_{k} = 0 \cdot b + q_{k}$
                \end{itemize}
    \item Consider then that $q_{k} = r_{k+1}$
    \item And that $b^{k+2} > n > b^{k+1} \rightarrow (k+2) \cdot log(b) \leq log(n) \leq (k+1) \cdot log(b)$.
    \item $\therefore k = O(\frac{log(n)}{log(b)})$.
\end{itemize}
We can now proceed with the computation of the cost of this operation: \newline
$T(n_{b}) = T(\#Divisions \cdot q_{i} \bmod b) = O(\frac{log(n)}{log(b)} \cdot log(n) \cdot log(b)) = $ \\
$O(log^{2}(n))$

\subsection{How to use Bezout formula to compute modular inverses}
An efficient way of computing the modular inverse of a given number $a$ with in the group $\mathbb{Z}{m}^{*}$ uses the corollary of the \emph{Bezout identity} and the \emph{Extended Euclidean Algorithm}.\newline
That is, given $a \cdot x \equiv_{m} 1$, we want to compute $x$.\newline
\emph{Extended Euclidean Algorithm}, given $a, m$ computes the $gcd(a,m)$ and also returns the coefficients $x,y$ for which $ax + my = 1$. \newline
At this point, the modular inverse of $a$ in $\mathbb{Z}{m}^{*}$ is $x$:
\begin{itemize}
    \item Let's consider that $ax + my \equiv_{m} 1$;
    \item Since $my \equiv_{m} 0$, then $ax \equiv_{m} 1$;
    \item $\therefore x \bmod n$ is the modular inverse of $a$ in $\mathbb{Z}{m}^{*}$ for its definition.
\end{itemize}
The complexity of this operation is then $O(log(x)log(n))$, because we have to compute the remainder of the division between $x$ and $n$ (this does not take into account the execution of the \emph{Extended Euclidean Algorithm}).

\subsection{Computing the order of an element in a cyclic group}
The order of an element $a$ in $\mathbb{Z}{p}^{*}$ ($m = order(a)$) is the minimum $m$ such that $a^{m} \equiv_{p} 1$.\newline
This problem is computationally hard, because the most efficient way to compute $order(a)$ is to brute force its value.\newline
The only optimization available is that we don't have to compute the modulars powers of $a$ from scratch each time, but we can save the results at each iteration. Therefore, at each step we can only compute the modular product $(a^{p-1} \cdot a) \bmod p$, that has a cost $O(log^{2}(p))$.
The cost of this algorithm is then $O(order(a) \cdot log^{2}(p))$, because we have to compute $a^{i} \bmod p$ for each attempt to find $order(a)$.

\subsection{Extended Euclidean Algorithm}
The Extended Euclidean Algorithm is a variation of the classic Euclidean Algorithm, that computes the GCD between two numbers $a, b$. \newline
It also provides the coefficients $\lambda, \mu$ such that $\lambda \cdot a + \mu \cdot b = GCD(a,b)$.\newline
\RestyleAlgo{ruled}
\begin{algorithm}
\KwData{$a, b$}
\KwResult{$(\lambda, \mu, GCD(a,b))$}
\caption{The Extended Euclidean Algorithm}\label{alg:ExtEucAlg}
$old\_r \gets a$\;
$r \gets b$\;
$old\_s \gets 1$\;
$s \gets 0$\;
$old\_t \gets 0$\;
$t \gets 1$\;
\While{$r \neq 0$}{
    $quotient \gets floor(old\_r / r)$\;
    $old\_r \gets r$\;
    $old\_s \gets s$\;
    $old\_t \gets t$\;

    $r \gets old\_r - quotient \cdot r$\;
    $s \gets old\_s - quotient \cdot s$\;
    $t \gets old\_t - quotient \cdot t$\;
    }
\Return{(s,t,old_r)}
\end{algorithm}
This algorithm has a cost $O(log^{3}(max\{a, b \}))$.

\subsection{Computation of square and m-th root of n}
The following algorithm can be used to compute efficiently $\lfloor \sqrt[m]{n} \rfloor$.\newline
It is assumed that the length of the result is known and is $l$.\newline
\RestyleAlgo{ruled}
\begin{algorithm}
\KwData{$n, m$}
\KwResult{$\lfloor \sqrt[m]{n} \rfloor$}
\caption{The Efficient m-th root of n}\label{alg:m_root}
$x_{0} \gets 2^{l - 1}$\;
\For{$i \gets 1$ \KwTo $l - 1$}{
    $x_{i} \gets x_{i-1} + 2^{l - i - 1}$\;
    \If{$x_{i}^{m} > n$}{
        $x_{i} \gets x_{i-1}$\;
    }
}
\Return{$x_{l-1}$}
\end{algorithm}
Let's consider the cost of this algorithm:
\begin{itemize}
    \item Computing $x_{i}^{m}$ has cost $O(log^{2}(n))$
    \item Comparing $x_{i}^{m}$ and $n$ has cost $O(log(n))$.
    \item The length of the loop is $O(log(n))$ iterations.
    \item The total cost is therefore $O(log^{3}(n))$.
\end{itemize}

\subsection{Compute $n, m$ given $n^{m}$}
We can extract the base and the exponent of an integer by making different attempts.\newline
Let's consider the cost of this operation:
\begin{itemize}
    \item We have to make at most $m$ attempts by brute force;
    \item At each attempt we have to compute $\lfloor sqrt[m_{i}]{n^{m}} \rfloor$;
    \item This operation has total cost of $\sum_{m=3}^{log(n)} O(\frac{log(n)}{m} \cdot log^{2}(m) \cdot log(m)) + O(log^{3}(n))$\footnote{$\frac{log(n)}{m}$ is the length of the loop};
    \item That is equal to $O(log^{3}(n)) \sum_{m=3}^{log(n)} O(\frac{log^{3}(m)}{m}) + O(log^{3}(n))$;
    \item $\sum_{m=3}^{log(n)} O(\frac{log^{3}(m)}{m})$ can be approximated by calculating the correspondant integral, to $O(loglog(n))$.
    \item The final cost is therefore $O(log^{3}(n) \cdot (loglog(n))^{2})$ \\
    $= O(log^{3 + \epsilon})$, with $\epsilon \in (0,1)$
\end{itemize}


\chapter{Elements of Number Theory}

\section{Definitions of Number Theory}
\subsection{The cyclic group $\mathbb{Z}_{n}^{*}$}
The cyclic group $\mathbb{Z}_{n}^{*}$ is defined as $\{a \in \mathbb{Z}_{n}: (a,n) = 1\}$.\newline
The \textbf{generator} of $\mathbb{Z}_{n}^{*}$ is a number in $\mathbb{Z}_{n}^{*}$ such that
$\forall a \in \mathbb{Z}_{n}^{*} \exists i: a \equiv_{m} g^{i}$.
For this reason, $\mathbb{Z}_{n}^{*}$ is also referred as $< g >$.\newline
An interesting property is that only for $[1,2,4, \phi, \phi^{\alpha}, 2 \phi^{\alpha}]$,
 $\mathbb{Z}_{n}^{*}$ is cyclic, where $\phi$ is a prime number and $\phi^\alpha$ is a power of a prime number.\newline

\subsection{Pseudoprime number}
For an integer $a > 1$, if a composite integer $x$ divides $ax-1 - 1$, then $x$ is called a \textbf{Fermat pseudoprime} to base $a$.\newline
 In other words, a composite integer is a Fermat pseudoprime to base a if it successfully passes the Fermat primality test for the base $a$.

\section{Reminders of Modular Arithmetic}
\subsection{Little Fermat's Theorem}\label{little_fermat_th}
\begin{theorem}[Little Fermat's Theorem]
    Consider what follows:
    \begin{itemize}
        \item Let $p$ be a prime number and $p \nmid a$.
        \item Then, $a^{p-1} \equiv_{p} 1$.
    \end{itemize}
\end{theorem}
\begin{proof}
    \begin{itemize}
        \item Let's consider $A = \{na \bmod p: n \in [1, p-1]\}$
        \item $A$ has all distinct elements due to the Lemma~\ref{bijection_lemma}.
        \item Then, $A = \mathbb{Z}_{n}^{*}$.
        \item Then, $\Pi_{n \in A} n \equiv_{p} \Pi_{n=1}^{p-1} na \Rightarrow (p-1)! \equiv_{p} (p-1)!a^{p-1}$
        \item Therefore, $a^{p-1} \equiv_{p} 1$ for the Wilson's Theorem~\ref{wilson_th}
    \end{itemize}
\end{proof}

\subsection{Wilson's Theorem}\label{wilson_th}
\begin{theorem}[Wilson's Theorem]
    Consider what follows:
    \begin{itemize}
        \item Let $n \in \mathbb{N} \setminus \{0,1\}$
        \item Then, $(n-1)! \equiv_{n} -1$.
    \end{itemize}
\end{theorem}
\begin{proof}
    The proof is by induction on $n$:
    \begin{itemize}
        \item \textbf{Base case}: $n = 2$\\
        Trivially, $1! \equiv_{2} -1$
        \item \textbf{Inductive step}:\\
        The theorem is assumed to be true up until $n - 1$. Let's consider the case of $n$:\\
        \begin{itemize}
            \item Consider the polynomial $g(x) = (x-1)(x-2) \dots (x - (n-1))$.
            \item $g$ has degree $p-1$ and costant term $(p-1)!$. It's roots are $[1,p-1]$.
            \item Consider $h(x) = x^{p-1} - 1$. $h$ has also degree $p-1$ and leading term $x^{p-1}$.
            \item Let $f(x) = g(x) - h(x)$.
            \item Then, $f$ has degree at most $p - 2$ (since the leading terms cancel), and modulo $p$ also has the $n - 1$ roots $1, 2, ..., n - 1$.
            \item But Lagrange's theorem says it cannot have more than $p - 2$ roots.
            \item $\therefore f \equiv_{n} 0$.
            \item Its costant term, $(n-1)! + 1 \equiv_{n} 0 \iff (n-1)! \equiv_{n} -1$
        \end{itemize}
    \end{itemize}
\end{proof}

\subsection{Euler-Fermat's Theorem}\label{euler_fermat_th}
\begin{theorem}[Euler-Fermat's Theorem]
    Consider what follows:
    \begin{itemize}
        \item Let $n \in \mathbb{Z}$ be a number.
        \item Then, $a^{\varphi(n)} \equiv_{n} 1$.
    \end{itemize}
\end{theorem}

\subsection{BÃ©zout's identity}
\begin{theorem}[Bezout's identity]
    Consider what follows:
    \begin{itemize}
        \item Let $a$ and $b$ be integers with greatest common divisor $d$.
        \item Then there exist integers $x$ and $y$ such that $ax + by = d$.
        \item Moreover, the integers of the form $az + bt$ are exactly the multiples of $d$.
    \end{itemize}
\end{theorem}

\subsection{Chinese Reminder's Theorem}
\begin{theorem}[Chinese Reminder's Theorem]
    Given a system of congruences:
    \begin{itemize}
        \item $x \equiv_{m_{1}} a_{1}$
        \item $x \equiv_{m_{2}} a_{2}$
        \item \dots
        \item $x \equiv_{m_{r}} a_{r}$
    \end{itemize}
    In which each module is prime with each others, that is $\forall i \neq j: (m_{i}, m_{j}) = 1$,
    then \textbf{there exists a simultaneous solution $x$ to all of the congruences}, and \textbf{any two solutions are congruent to one another modulo $M = m_{1}m_{2}\dots m_{r}$}.
\end{theorem}

\begin{proof}
    Consider what follows:
    \begin{itemize}
        \item Suppose that $x'$ and $x"$ are two solutions.
        \item Let $x = x' - x''$.
        \item Then x must be congruent to $0$ modulo each $m_{i}$ and hence modulo $M$.
        \item Let $M_{i} = M/m_{i}$, to be the product of all of the moduli except for the i-th.
        \item Then $GCD(m_{i}, M_{i}) = 1$ and therefore $\exists N_{i}: M_{i}N_{i} \equiv_{m_{i}} 1$.
        \item Set $x = \sum_{i}a_{i}M_{i}N_{i}$;
        \item Then, for each $i$ we see that the terms in the sum other than the i-th term are all divisible by $m_{i}$, because $m_{i} | M_{j}$ when $i \neq j$.
        \item Thus, for each $i$ we have: $x \equiv_{m_{i}} a_{i}M_{i}N_{i} \equiv_{m_{i}} a_{i}$,
    \end{itemize}
\end{proof}

\subsection{Bijection of a modular function Lemma}
\begin{lemma}[Bijection of a modular function Lemma]\label{bijection_lemma}
    Consider $a \in \mathbb{Z}_{n}^{*} and f: \mathbb{Z}_{n} \rightarrow \mathbb{Z}_{n}$.\newline
    Then, $f$ is a bijection.\newline
\end{lemma}
\begin{proof}
    Part 1: $f$ is injective.\newline
    \begin{itemize}
        \item Assume $f(x_{1}) = f(x_{2}) \in \mathbb{Z}_{n} \iff ax_{1} \equiv_{n} ax_{2}$.
        \item Then $\exists k \in \mathbb{Z}: ax_{1} - ax_{2} = kn$.
        \item Therefore, $a(x_{1} - x_{2}) = kn \iff a^{-1}a(x_{1} - x_{2}) = a^{-1}kn$\\
        $\iff (x_{1} - x_{2}) = a^{-1}kn \iff x_{1} - x_{2} \equiv_{m} 0$.
        \item $\therefore x_{1} \equiv_{m} x_{2}$, so $f$ is an injection.
    \end{itemize}
    Part 2: $f$ is surjective.
    \begin{itemize}
        \item Let $b \in \mathbb{Z}_{n}$.
        \item Let $\overline{x} = a^{-1}b \in \mathbb{Z}_{n}$.
        \item Then, $f(\overline{x}) \equiv_{m} a \cdot \overline{x}$\\
        $\equiv_{m} a \cdot a^{-1} \cdot b \equiv_{m} b$.
        \item Therefore, $f$ is surjective.
    \end{itemize}
    Since $f$ is injective $\land$ $f$ is surjective, then $f$ is bijective.
\end{proof}

\subsection{Euler's $\varphi$ function Lemmas}
\begin{lemma}[Sum of the prime divisors' $\varphi$-function]
    $\forall n \in \mathbb{N} \backslash \{0\}: \sum_{\frac{n}{d}} \varphi(d) = n$, where $\frac{n}{d}$ is the set of prime divisors of $n$.\newline
\end{lemma}
\begin{proof}
    \begin{itemize}
        \item Let $B$ be $\{\frac{h}{n}: h \in \mathbb{Z}_{n} \land n \in \mathbb{N} \}$.
        \item Therefore, $B = \cup_{\frac{n}{d}}\{a \in \{1, \dots, n\} \land (a,d) = 1\}$.
        \item Then, $n = |B| = \sum_{\frac{d}{n}} \varphi(d)$.
        \item Consider $(a_{1}, d_{1}) = 1 \land (a_{2}, d_{2}) = 1$\\, where $d_{1}|n \land d_{2}|n$.
        \item Then, $\frac{a_{1}}{d_{1}} = \frac{a_{2}}{d_{2}} \iff a_{1}d_{2} = a_{2}d_{1}$.
        \item Then, $d_{1}|a_{1}d_{2} \Rightarrow d_{1}|d_{2} \land$\\
        $d_{2}|a_{2}d_{1} \Rightarrow d_{2}|d_{1}$.
        \item $\therefore d_{1} = d_{2}$. This is clearly a contradiction, because in $B$ each divisor is counted once.
    \end{itemize}
\end{proof}

\begin{lemma}[Number of divisors of a prime number's power]
    Let $p \in \mathbb{N}$  be a prime number, and $\alpha \in \mathbb{N}$.\\
    Then, $\varphi(p^{\alpha}) = p^{\alpha - 1}(p - 1)$.\newline
\end{lemma}
\begin{proof}
    The proof is by induction on $\alpha$.\newline
    \textbf{Case base:} $\alpha = 1$\\
    Then, $p = \sum_{\frac{d}{p}} \varphi(d) = \varphi(1) + \varphi(p) = 1 + \varphi(p)$\\
    $\Rightarrow \varphi(p) = p - 1$.\newline
    \textbf{Case base:} $\alpha = 2$\\
    Then, $p^{2} = \sum_{\frac{d}{p^{2}}} \varphi(d) = \varphi(1) + \varphi(p) + \varphi(p^{2})$\\
    $= 1 + p - 1 + \varphi(p^{2}) \Rightarrow \varphi(p^{2}) = p^{2} - p - 1 + 1$\\
    $= p \cdot (p - 1) = p^{\alpha - 1}(p - 1)$\newline
    \textbf{Inductive Step:} we can now assume that $\varphi(p^{\alpha}) = p^{\alpha - 1}(p - 1)$ up until $\alpha - 1$. We'll proceed now to demonstrate that this is also valid for each $\alpha$.\\
    $p^{\alpha} = \sum_{\frac{d}{p^{\alpha}}} \varphi(d)$\\
    $= \sum_{i = 0}^{\alpha - 1} \varphi(p^{i}) + \varphi(p^{\alpha})$\\
    $\Rightarrow \varphi(p^{\alpha}) = p^{\alpha} - p^{\alpha - 1} = p^{\alpha - 1}(p - 1)$.\newline
\end{proof}

\subsection{Multiplicativity of the Euler's $\varphi$-function}
\begin{theorem}[Multiplicativity of the Euler's $\varphi$-function]
    Let's consider the Euler's function $\varphi(n) = |\mathbb{Z}_{n}^{*}$.\\
    Let's consider that $n = \prod_{i=1}^{r} p_{i}^{\alpha_{i}}$, where $p_{i}$ is a prime number, and $\alpha_{i} \in \mathbb{N}$.\\
    Then, $=$
\end{theorem}
\begin{proof}
\end{proof}

\chapter*{Useful Facts}
\begin{itemize}
    \item The \href{https://gmplib.org/}{GMP library} is a free library for arbitrary precision arithmetic. It implements all the basic arithmetic operations with the maximum efficency possible.
\end{itemize}

\listofalgorithms
\listoftheorems[ignoreall, show={theorem}]

\begingroup               % Temporarily disable \clearpage to show both lists on one page
  \let\clearpage\relax    % http://tex.stackexchange.com/a/14511/104449
  \renewcommand{\listtheoremname}{List of Lemmas}
  \listoftheorems[ignoreall, show={lemma}]
\endgroup

\end{document}
