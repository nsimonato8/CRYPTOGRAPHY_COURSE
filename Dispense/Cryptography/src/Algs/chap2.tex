\section{Notation}
\begin{itemize}
    \item Let $b$ be a numeric base.
    \item Let $n$ be a number in $N$.
    \item Length of a number: $l_{b}(n)$, $k$. It's equal to $\operatorname{log}(n)$.
    \item $(a,b)$ is the Maximum Common Divisor of $a,b$.
    \item Let $n \in N$: $n = (d_{k-1}, d_{k-2}, \dots, d_{1}, d_{0})$\footnote{$d_{k-1} \neq 0$}.
    \item $\varphi(n)$: the number of elements $a$ in $[1,n]$ such that $(a,n) = 1$.
    \item $\equiv_{p}$ is the equivalence in base $p$. Ex.: $5 \equiv_{3} = 5 \bmod 3 = 2 $.
    \item Let $\mathbb{Z}_{n}[X]$ be the set of polynomials in $X$ with coefficients in $\mathbb{Z}_{n}$.
\end{itemize}


\section{Classification of the algorithms' complexity}
In order to better identify the classes of complexity of the algorithms, the following 3 classes are defined:
\begin{itemize}
    \item Polynomial time: $O(\operatorname{log}^{\alpha}(n))$ bit operations, where $\alpha > 0$.
    \item Exponential time: $O(exp(c \cdot \operatorname{log}(n)))$ bit operations, where $c > 0$.
    \item Sub-exponential time: $O(exp(c \cdot \operatorname{log}(n))^{\alpha})$ bit operations, where $c > 0, \alpha \in ]0, 1[$.
\end{itemize}



\section{Basic bit operations}
\subsection{Sum of 3 bits - 3-bit-sum}
Given $n_{1}, n_{2}$ their sum produces $n_{1} + n_{2}$ and their carry. \newline
Since $n_{1}, n_{2} \in [0,1]$, then this operation can be done in $O(1)$.

\subsection{Summation of 2 numbers}
Given $n_{1}, n_{2}$ their sum produces $n_{1} + n_{2}$. \newline
Since the sum is computed bit by bit, the 3-bit-sum is performed \[\operatorname{max}\{\operatorname{lenght}(n_{1}), \operatorname{length}(n_{2})\}\] times.\newline
Each time the carry-on of the previous sum is added to the two digits. \newline
This operation has then complexity:
\[O(\operatorname{max}\{lenght(n_{1}), \operatorname{length}(n_{2})\}) = O(\operatorname{max}\{\operatorname{log}(n_{1}), \operatorname{log}(n_{2})\})\]

\subsection{Summation of $n$ numbers}
The summation of $n$ numbers is simply the sum of two numbers, but performed $n - 1$ times. \newline
Let's assume that: \[\forall i \in [1,n]: M \geq a_{i}\]
The complexity of this operation is then: \[O((n-1) \cdot \operatorname{log}(M)) = O(n)\] \newline

\subsection{Product of 2 numbers}
If we consider the classic implementation of the binary multiplication, that is just a sequence of summations. \newline
\begin{itemize}
    \item The number of summations to execute is equal to the length of the smallest number, $O(\operatorname{log}(n))$.
    \item The maximum cost of a single summation is $O(\operatorname{log}(m))$.
    \item Then, $T(m \cdot n) = O(\operatorname{log}(m) \cdot \operatorname{log}(n))$, but, if we consider the worst case\footnote{two numbers that are equally large}, that becomes $O(\operatorname{log}^{2}(m))$.
\end{itemize}

\subsection{Division of 2 numbers}
Let's consider the division of two numbers $m, n$. This operation consists in \\ finding two numbers $q, r$ such that $m = q \cdot n + r$. \newline
This is achieved by performing a succession of subtractions, until the ending condition $0 \leq r < n$ is reached. \newline
\begin{itemize}
    \item Let's consider that the number of steps of this algorithm is $O(\operatorname{log}(q))$.
    \item Moreover: \[q \leq m \therefore \#steps = O(\operatorname{log}(m))\]
    \item It's assumed that the cost of the single subtraction is $O(\operatorname{log}(n))$.
    \item Then: \[T(\frac{m}{n}) = O(\operatorname{log}(n) \cdot \operatorname{log}(m))\]
\end{itemize}

\subsection{Production of $n$ numbers}
Let's assume that: \[j \in [1, s+1] \text{ and } M = \operatorname{max}(m_{j})\]
The cost of the operation $\prod_{j = 1}^{s+1} m_{j}$ is then $O(s^{2} \cdot \operatorname{log}^{2}(M))$. This will now be considered our inductive hypothesis.\newline
Proof by induction, on $s$:
\begin{itemize}
    \item (1) \textbf{Base case}: \[T(m_{1} \cdot m_{2}) = O(\operatorname{log}(m_{1}) \cdot \operatorname{log}(m_{2})) = O(k_{1} \cdot k_{2}) \leq c \cdot k_{M}^{2}\]
    \item (2) \textbf{Base case}:
    \begin{align*}
        T(m_{1} \cdot m_{2} \cdot m_{3}) & = T(m_{1} \cdot m_{2}) + T((m_{1} \cdot m_{2}) \cdot m_{3}) \\
        & \leq c \cdot k_{M}^{2} + c \cdot k_{m_{1} \cdot m_{2}} + k_{m_{3}} \\
        & \leq c \cdot k_{M}^{2} + c \cdot k_{M^{2}} + k_{M}
    \end{align*}
    \item Inductive step: we assume the inductive hypothesis to be true up to $s$. Then:
    \begin{align*}
        T(\prod_{j = 1}^{s+1} m_{j}) & = T([\prod_{j = 1}^{s} m_{j}] \cdot m_{s+1}) \\
        & \leq c \cdot \sum_{j=1}^{s} (j \cdot k_{M}^{2}) \\
        & = c \cdot k_{M}^{2} \cdot \frac{s \cdot (s-1)}{2} \\
        & = O(k_{M}^{2} \cdot s^{2}) \\
        & = O(s^{2} \cdot \operatorname{log}^{2}(M)) \\
    \end{align*}
\end{itemize}
\subsubsection{Applications}
\begin{itemize}
    \item An analogous dimonstration can be used to prove that \[T(\prod_{j = 1}^{s+1} m_{j} \bmod n) = O(s \cdot \operatorname{log}^{2}(M))\]
    \item This proof can be used to show that \[T(m!) = O(m \cdot \operatorname{log}^{2}(m))\]
\end{itemize}

\section{Optimizations of more complex operations}
\subsection{Powers \& Modular Powers}
Let's consider what follows: \[a^n = a \cdot a \cdot a \cdot \dots \cdot a\]
Where $a$ is repeated $n$ times.
\subsubsection{Trivial implementation}
The most trivial implementation would consists in computing the product $\prod_{j = 1}^{n} a$. This would imply a cost of $O(n^{2} \cdot \operatorname{log}^{2}(a)))$.\newline
What follows is a suggestion that could improve the cost of this operation.
\subsubsection{Square \& Multiply method for scalars, modular powers}
Each number in $\mathbb{Z}$ can be represented in a binary notation. \newline
Let's consider
\[n = (b_{k-1}, b_{k-2}, \dots, b_{0}) = \sum_{i=0}^{k-1} b_{i} \cdot 2^{i}\]
It is clear that we can spare a lot of computational resources by just calculating the powers of 2 and summing the ones that have $b_{i} = 1$. The following algorithm explains the procedure in detail.
\RestyleAlgo{ruled}
\begin{algorithm}
\caption{The Square \& Multiply Method}\label{alg:SquareMultiply}
$P \gets 1$\;
$M \gets m$\;
$A \gets a \bmod n$\;\label{instr_3}
\While{$M > 0$}{
    $q \gets \lfloor \frac{M}{2} \rfloor$\;\label{instr_5}
    $r \gets M - s \cdot q$\;\label{instr_6}
    \If{$r = 1$}{
        $P \gets P \cdot A \bmod n$\;\label{instr_8}
    }
    $A \gets A^{2} \bmod n$\;\label{instr_10}
    $M \gets q$\;

}
\Return{P}
\end{algorithm}
Let's compute the complexity of this algorithm:
\begin{itemize}
    \item All of the assignments $X \gets Y$ are implemented in $O(\operatorname{log}(Y))$.
    \item The cost of \ref{instr_3} is $O(\operatorname{log}(a) \cdot \operatorname{log}(n))$, because it ensures that $A \leq n$.
    \item Instructions \ref{instr_5} and \ref{instr_6} can be executed in $O(\operatorname{log}(m))$.
    \item Instrucion \ref{instr_8} can be executed in $O(\operatorname{log}^{2}(n))$.
    \item The cost of \ref{instr_10} is $O(\operatorname{log}^{2}(n))$, because it ensures that $A \leq n$.
    \item The loop is executed $\operatorname{log}(m)$ times.
    \item The total cost of this algorithm is then $O(\operatorname{log}(n) \cdot \operatorname{log}(a) + \operatorname{log}(m) \cdot (\operatorname{log}^{2}(n) + \operatorname{log}(m)))$ \\ $= O(\operatorname{log}^{2}(m) + \operatorname{log}(m) \cdot \operatorname{log}(n))$.
\end{itemize}
This algorithm can be easily converted for the computation of non-modular powers by applying the following changes:
\RestyleAlgo{ruled}
\begin{algorithm}
    $A \gets a \bmod n$ $\Longrightarrow$ $A \gets a$\;\label{instr_3a}
    $P \gets P \cdot A \bmod n$ $\Longrightarrow$ $P \gets P \cdot A$\;\label{instr_8a}
    $A \gets A^{2} \bmod n$ $\Longrightarrow$ $A \gets A^{2}$\;\label{instr_10a}
\end{algorithm}
\subsubsection{Square \& Multiply method for polynomials}
Let's consider $\Re = \frac{\mathbb{Z}_{n}[x]}{x^{r} - 1}$. The modular powers of the elements in this set can be computed by using a variation of the Square \& Multiply method.
\begin{itemize}
    \item Assume that $f, g \in \Re$.
    \item Let \[h(x) = f(x) \cdot g(x) = \sum_{j = 0}^{2r - 2} h_{j} \cdot x^{j}\]
    \item Where \[h(j) = (\sum_{i=0}^{j} f_{i} \cdot g_{j - i} \bmod n) \bmod n\]
    \item Then: \[T(h_{j}) = O(j \cdot \operatorname{log}^{2}(n))\]
    \item Then: \[T(h(x)) = O(\sum_{j = 0}^{\operatorname{log}^{2}(n)}) = O(r^{2} \cdot \operatorname{log}^{2}(n))\]
\end{itemize}
This result will be useful in the following computations. \newline
Let's now consider $\frac{h(x)}{x^{r} - 1}$.
\begin{itemize}
    \item When $j > r - 1$, $h_{j} \cdot x^{j}$ does not take any part in the computations.
    \item When $j = r$, then: \[\frac{h_{r} \cdot x^{r}}{x^{r} - 1 } = h_{r} + \frac{h_{r}}{x^{r} - 1}\]
    Or, in other words: $h_{r} \cdot x^{r} \equiv_{x^{r} - 1} h_{r}$.
    \item In the other cases: \[h_{r+i} \cdot x^{r+i} \equiv_{x^{r} - 1} h_{r-i} \cdot x^{r-i} \text{ for } 1 \leq i \leq r - 2\]
\end{itemize}
%An image can be useful
Then:
\[h(x) \equiv_{(n, x^{r} - 1)} f(x) \cdot g(x) \equiv
[\sum_{j=0}^{r - 2}((h_{j} + h_{r + j}) \bmod n) \cdot x^{j}] + h_{r-1} \cdot x^{r-1}\]\label{comp:partial_prod_pol}

Therefore: \[T(h(x) \bmod (n, x^{r} - 1)) = O(r^{2} \cdot \operatorname{log}^{2}(n))\]
Finally, we can analyze the complexity of the computation of the modular power $h(x)$ elevated to $n$. \newline
In order to optimize the use of the computational resources, we can use a variation of the Square \& Multiply method (See \ref{alg:SquareMultiply});
although, this time, the computation of the partial products will be conducted by using the previously explained procedure (See \ref{comp:partial_prod_pol}). \newline
The cost of this method would then be
\begin{align*}
    T(\#Loops \cdot (h(x) \bmod (n, x^{r} - 1))) = O(\operatorname{log}(n) \cdot r^{2} \cdot \operatorname{log}(n)) \\
    & = O(r^{2} \cdot \operatorname{log}^{3}(n))
\end{align*}



\subsection{Finding the $b$-expansion of $n$ ($n_{b}$)}
Let's consider the cost in bit operations of the conversion of a number $n$ to a new base $b$. \newline
The algorithm used will be the classical: a succession of divisions by $b$. \newline
\begin{itemize}
    \item Let's consider \[r_{i} \in \{0, 1, \dots, b-1\}\]
    \item Let \[n_{b} = (r_{k+1}, r_{k}, \dots, r_{1}, r_{0})\]
    \item Then: \begin{align*}
                    n &= q_{0} \cdot b + r_{0} \\
                    q_{0} &= q_{1} \cdot b + r_{1} \\
                    \dots \\
                    q_{k} &= 0 \cdot b + q_{k}
                \end{align*}
    \item Consider then that \[q_{k} = r_{k+1}\]
    \item And that \[b^{k+2} > n > b^{k+1} \iff (k+2) \cdot \operatorname{log}(b) \leq \operatorname{log}(n) \leq (k+1) \cdot \operatorname{log}(b)\]
    \item Therefore, \[k = O(\frac{\operatorname{log}(n)}{\operatorname{log}(b)})\]
\end{itemize}
We can now proceed with the computation of the cost of this operation:
\begin{align*}
    T(n_{b}) &= T(\#Divisions \cdot q_{i} \bmod b)\\
    & = O(\frac{\operatorname{log}(n)}{\operatorname{log}(b)} \cdot \operatorname{log}(n) \cdot \operatorname{log}(b)) \\
    &= O(\operatorname{log}^{2}(n))
\end{align*}


\subsection{How to use Bezout formula to compute modular inverses}
An efficient way of computing the modular inverse of a given number $a$ with in the group $\mathbb{Z}_{m}^{*}$ uses the corollary of the \emph{Bezout identity} and the \emph{Extended Euclidean Algorithm}.\newline
That is, given $a \cdot x \equiv_{m} 1$, we want to compute $x$.\newline
\emph{Extended Euclidean Algorithm}, given $a, m$ computes the $gcd(a,m)$ and also returns the coefficients $x,y$ for which $ax + my = 1$. \newline
At this point, the modular inverse of $a$ in $\mathbb{Z}_{m}^{*}$ is $x$:
\begin{itemize}
    \item Let's consider that $ax + my \equiv_{m} 1$;
    \item Since $my \equiv_{m} 0$, then $ax \equiv_{m} 1$;
    \item $\therefore x \bmod n$ is the modular inverse of $a$ in $\mathbb{Z}_{m}^{*}$ for its definition.
\end{itemize}
The complexity of this operation is then $O(\operatorname{log}(x)\operatorname{log}(n))$, because we have to compute the remainder of the division between $x$ and $n$ (this does not take into account the execution of the \emph{Extended Euclidean Algorithm}).

\subsection{Computing the order of an element in a cyclic group}
The order of an element $a$ in $\mathbb{Z}_{p}^{*}$ ($m = order(a)$) is the minimum $m$ such that $a^{m} \equiv_{p} 1$.\newline
This problem is computationally hard, because the most efficient way to compute $\operatorname{order}(a)$ is to brute force its value.\newline
The only optimization available is that we don't have to compute the modulars powers of $a$ from scratch each time, but we can save the results at each iteration. Therefore, at each step we can only compute the modular product $(a^{p-1} \cdot a) \bmod p$, that has a cost $O(\operatorname{log}^{2}(p))$.
The cost of this algorithm is then $O(order(a) \cdot \operatorname{log}^{2}(p))$, because we have to compute $a^{i} \bmod p$ for each attempt to find $order(a)$.

\subsection{Extended Euclidean Algorithm}
The Extended Euclidean Algorithm is a variation of the classic Euclidean Algorithm, that computes the GCD between two numbers $a, b$. \newline
It also provides the coefficients $\lambda, \mu$ such that $\lambda \cdot a + \mu \cdot b = \operatorname{GCD}(a,b)$.\newline
\RestyleAlgo{ruled}
\begin{algorithm}
\KwData{$a, b$}
\KwResult{$(\lambda, \mu, GCD(a,b))$}
\caption{The Extended Euclidean Algorithm}\label{alg:ExtEucAlg}
$old\_r \gets a$\;
$r \gets b$\;
$old\_s \gets 1$\;
$s \gets 0$\;
$old\_t \gets 0$\;
$t \gets 1$\;
\While{$r \neq 0$}{
    $quotient \gets floor(old\_r / r)$\;
    $old\_r \gets r$\;
    $old\_s \gets s$\;
    $old\_t \gets t$\;

    $r \gets old\_r - quotient \cdot r$\;
    $s \gets old\_s - quotient \cdot s$\;
    $t \gets old\_t - quotient \cdot t$\;
    }
\Return{(s,t,old_r)}
\end{algorithm}
This algorithm has a cost $O(\operatorname{log}^{3}(max\{a, b \}))$.

\subsection{Computation of square and m-th root of n}
The following algorithm can be used to compute efficiently $\lfloor \sqrt[m]{n} \rfloor$.\newline
It is assumed that the length of the result is known and is $l$.\newline
\RestyleAlgo{ruled}
\begin{algorithm}
\KwData{$n, m$}
\KwResult{$\lfloor \sqrt[m]{n} \rfloor$}
\caption{The Efficient m-th root of n}\label{alg:m_root}
$x_{0} \gets 2^{l - 1}$\;
\For{$i \gets 1$ \KwTo $l - 1$}{
    $x_{i} \gets x_{i-1} + 2^{l - i - 1}$\;
    \If{$x_{i}^{m} > n$}{
        $x_{i} \gets x_{i-1}$\;
    }
}
\Return{$x_{l-1}$}
\end{algorithm}
Let's consider the cost of this algorithm:
\begin{itemize}
    \item Computing $x_{i}^{m}$ has cost $O(\operatorname{log}^{2}(n))$
    \item Comparing $x_{i}^{m}$ and $n$ has cost $O(\operatorname{log}(n))$.
    \item The length of the loop is $O(\operatorname{log}(n))$ iterations.
    \item The total cost is therefore $O(\operatorname{log}^{3}(n))$.
\end{itemize}

\subsection{Compute $n, m$ given $n^{m}$}
We can extract the base and the exponent of an integer by making different attempts.\newline
Let's consider the cost of this operation:
\begin{itemize}
    \item We have to make at most $m$ attempts by brute force;
    \item At each attempt we have to compute $\lfloor sqrt[m_{i}]{n^{m}} \rfloor$;
    \item This operation has total cost of \[\sum_{m=3}^{\operatorname{log}(n)} O(\frac{\operatorname{log}(n)}{m} \cdot \operatorname{log}^{2}(m) \cdot \operatorname{log}(m)) + O(\operatorname{log}^{3}(n))\]\footnote{$\frac{\operatorname{log}(n)}{m}$ is the length of the loop};
    \item That is equal to \[O(\operatorname{log}^{3}(n)) \sum_{m=3}^{\operatorname{log}(n)} O(\frac{\operatorname{log}^{3}(m)}{m}) + O(\operatorname{log}^{3}(n))\]
    \item \[\sum_{m=3}^{\operatorname{log}(n)} O(\frac{\operatorname{log}^{3}(m)}{m})\] can be approximated by calculating the correspondant integral, to $O(\operatorname{log}\operatorname{log}(n))$.
    \item The final cost is therefore
    \begin{align*}
        O(\operatorname{log}^{3}(n) \cdot(\operatorname{log}\operatorname{log}(n))^{2})
        = O(\operatorname{log}^{3 + \epsilon}) \text{, with } \epsilon \in (0,1)
    \end{align*}
\end{itemize}
